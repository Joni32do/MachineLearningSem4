{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: k-nearest neighbors (100 points)\n",
    "\n",
    "Only use the already imported libraries `numpy` and `matplotlib.pyplot` for the assignment. Do not import any other library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* *Louisa Pawusch, Maureen Kosiol, Maureen Jonathan Schnitzler* \n",
    "* *3408829, 3398926, 3465192* \n",
    "* *Simulation Technology*\n",
    "* *B.Sc.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required packages and dataset. Do not modify.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_wine_dataset():\n",
    "    from sklearn import datasets\n",
    "    wine = datasets.load_wine()\n",
    "    X = wine.data\n",
    "    y = wine.target\n",
    "    return X, y\n",
    "    \n",
    "X, y = load_wine_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Visualization and Preprocessing (25 points)\n",
    "\n",
    "1) *(5 Points)* Explain the content of the dataset in few words. What are the input features? What is the classification target? Check out: [http://archive.ics.uci.edu/ml/datasets/Wine).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of a chemical analysis of\n",
    "      wines grown in the same region in Italy but derived from three\n",
    "      different cultivars\n",
    "      \n",
    "The defining features are: \n",
    "      \n",
    "1) Alcohol\n",
    "2) Malic acid\n",
    " \t3) Ash\n",
    "\t4) Alcalinity of ash  \n",
    " \t5) Magnesium\n",
    "\t6) Total phenols\n",
    " \t7) Flavanoids\n",
    " \t8) Nonflavanoid phenols\n",
    " \t9) Proanthocyanins\n",
    "\t10)Color intensity\n",
    " \t11)Hue\n",
    " \t12)OD280/OD315 of diluted wines\n",
    " \t13)Proline  \n",
    "    \n",
    "The classification target is from which cultivar the wine is coming from\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) *(5 Points)* Compute and print the following statistics about the dataset:\n",
    "  - Number of samples\n",
    "  - Number of samples per class\n",
    "  - Mean and standard deviation for each input feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples 178\n",
      "Number of samples per class\n",
      "   Wines from cultivars  0  : 59\n",
      "   Wines from cultivars  1  : 71\n",
      "   Wines from cultivars  2  : 48\n",
      "Alcohol :\n",
      "    Mean Deviation:      13.000617977528083\n",
      "    Standard Deviation:  0.809542914528517\n",
      "Malic acid :\n",
      "    Mean Deviation:      2.336348314606741\n",
      "    Standard Deviation:  1.1140036269797895\n",
      "Ash :\n",
      "    Mean Deviation:      2.3665168539325854\n",
      "    Standard Deviation:  0.2735722944264325\n",
      "Alcalinity of ash :\n",
      "    Mean Deviation:      19.49494382022472\n",
      "    Standard Deviation:  3.330169757658213\n",
      "Magnesium :\n",
      "    Mean Deviation:      99.74157303370787\n",
      "    Standard Deviation:  14.242307673359807\n",
      "Total phenols :\n",
      "    Mean Deviation:      2.295112359550562\n",
      "    Standard Deviation:  0.6240905641965366\n",
      "Flavanoids :\n",
      "    Mean Deviation:      2.0292696629213474\n",
      "    Standard Deviation:  0.9960489503792328\n",
      "Nonflavanoid phenols :\n",
      "    Mean Deviation:      0.36185393258426973\n",
      "    Standard Deviation:  0.12410325988364797\n",
      "Proanthocyanins :\n",
      "    Mean Deviation:      1.5908988764044953\n",
      "    Standard Deviation:  0.5707488486199377\n",
      "Color intensity :\n",
      "    Mean Deviation:      5.058089882022473\n",
      "    Standard Deviation:  2.3117646609525573\n",
      "Hue :\n",
      "    Mean Deviation:      0.9574494382022468\n",
      "    Standard Deviation:  0.2279286065650725\n",
      "OD280/OD315 of diluted wines :\n",
      "    Mean Deviation:      2.6116853932584254\n",
      "    Standard Deviation:  0.7079932646716006\n",
      "Proline :\n",
      "    Mean Deviation:      746.8932584269663\n",
      "    Standard Deviation:  314.0216568419877\n"
     ]
    }
   ],
   "source": [
    "n = len(X)\n",
    "print(\"Number of samples\", n)\n",
    "print(\"Number of samples per class\")\n",
    "count = np.zeros(3, dtype = int)\n",
    "for i in y:\n",
    "    count[i] += 1\n",
    "for i in range(3):\n",
    "    print(\"   Wines from cultivars \",i,\" :\", count[i])\n",
    "\n",
    "feature = [\"Alcohol\",\"Malic acid\", \"Ash\",\"Alcalinity of ash\", \"Magnesium\",\"Total phenols\",\n",
    "           \"Flavanoids\",\"Nonflavanoid phenols\",\"Proanthocyanins\",\"Color intensity\",\"Hue\",\n",
    "           \"OD280/OD315 of diluted wines\",\"Proline\"]\n",
    "for i in range(13):\n",
    "    meanDev = 0\n",
    "    standDev = 0\n",
    "    for j in range(n):\n",
    "        meanDev += X[j][i]\n",
    "    meanDev = meanDev/n\n",
    "    for j in range(n):\n",
    "        standDev += (X[j][i] - meanDev)**2 \n",
    "    standDev = np.sqrt(standDev/n)\n",
    "    print(feature[i],\":\")\n",
    "    print(\"    Mean Deviation:     \", meanDev)\n",
    "    print(\"    Standard Deviation: \", standDev)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) *(5 Points)* Visualize the variables *alcohol* and *magnesium* in a scatter plot (*alcohol* on the x-axis, *magnesium* on the y-axis). Color each point of the plot according to its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x141573ead00>"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.transpose(X)[0], np.transpose(X)[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) *(5 Points)* Split the dataset randomly into training and test data. 70% of data should be used for training and 30% should be used for testing. Implement the function `train_test_split`. Do not modify the interface of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(X, y):\n",
    "    n1 = len(X)\n",
    "    k = int(0.7*n1)\n",
    "    ran = np.arange(n1)\n",
    "    np.random.shuffle(ran)\n",
    "    \n",
    "    X_train = np.zeros((k,13))\n",
    "    y_train = np.zeros((k,1), dtype = int)\n",
    "    X_test = np.zeros((n1-k,13))\n",
    "    y_test = np.zeros((n1-k,1), dtype = int)\n",
    "    \n",
    "    for i in range(n1):\n",
    "        if i < k:\n",
    "            X_train[i] = X[ran[i]]\n",
    "            y_train[i] = y[ran[i]]\n",
    "        else:\n",
    "            X_test[i-k] = X[ran[i]]\n",
    "            y_test[i-k] = y[ran[i]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns X_train, X_test, y_train, y_test, \n",
    "        where X_train and X_test are the input features of the training and test set,\n",
    "        and y_train and y_test are the class labels of the training and test set.\n",
    "    \"\"\"\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "assert (X_train.shape[0] + X_test.shape[0]) == X.shape[0]\n",
    "assert (y_train.shape[0] + y_test.shape[0]) == y.shape[0]\n",
    "assert X_train.shape[1] == X_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) *(5 Points)* kNN uses a distance measure to identify close neighbors. If the input features are not of the same scale, the distance is not as meaningful, which can negatively impact classification performance. Perform min-max scaling (i.e. scale the values of the input features in such a way that their range is from 0 to 1) on the training and test data. Remember that you should only use information from the training data to perform the scaling on both data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(np.shape(X_train)[1]): #all 13 features\n",
    "    ma = max(X_train[:,i])\n",
    "    mi = min(X_train[:,i])\n",
    "    X_train[:,i] = (X_train[:,i]-mi)/(ma-mi)\n",
    "    X_test[:,i] = (X_test[:,i]-mi)/(ma-mi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: k-nearest neighbors (50 Points)\n",
    "*Choose classes randomly if weights are equal for multiple classes*\n",
    "\n",
    "**For B.Sc. Data Science:**  \n",
    "\n",
    "Implement the kNN algorithm with uniform weighting and arbitrary `k`. Fill out the `predict` method of class `KNearestNeighbors`. \n",
    "\n",
    "Use Euclidean distance to determine the nearest neighbors.\n",
    "You can ignore the optional parameter `weights`, which is provided as a field in the kNN class.\n",
    "\n",
    "**For all students other than B.Sc. Data Science:**\n",
    "\n",
    "Implement the kNN algorithm with uniform and distance-based weighting and arbitrary `k`.\n",
    "Fill out the `predict` method of class `KNearestNeighbors`.\n",
    "\n",
    "The parameter `weights` will either contain the string `uniform` or `distance`. \n",
    "- If the value is `uniform`, the classifier should use the Euclidean distance for determining nearest neighbors and uniform weighting. \n",
    "- If the value is a `distance`, the classifier should use the Euclidean distance for determining neares neighbors and perform distance-weighted classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbors(object):\n",
    "    def __init__(self, k, weights='uniform'):\n",
    "        self.k = k\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        This functions saves the training data to be used during the prediction.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Returns a vector of shape (n,) if X has shape (n,d), \n",
    "        where n is the number of samples and d is the number of features.\n",
    "        \"\"\"\n",
    "        distances = np.zeros((len(self.X),))\n",
    "        predictions = np.zeros((len(X),), dtype = int)\n",
    "        for i in range(len(X)):\n",
    "                for j in range(len(self.X)):\n",
    "                    distances[j] = euclidean_distance(self.X[j], X[i])\n",
    "                idx = np.argpartition(distances, self.k)\n",
    "                idx = idx[0:(self.k)]\n",
    "                count = np.zeros((3,))\n",
    "                \n",
    "                if self.weights == \"uniform\":\n",
    "                    for k in idx:\n",
    "                        count[self.y[k]] += 1 #This is working because if y[k] is between 0 and 2 for the values of the cultivars\n",
    "                    \n",
    "                    \n",
    "                elif self.weights == \"distance\":\n",
    "                    #Weights are evaluated by using the simple inverse function, this could be more elaborated\n",
    "                    if distances[idx[0]] == 0: predictions[i] = y[idx[0]] #No division by zero error\n",
    "                    else:\n",
    "                        for k in idx:\n",
    "                            count[self.y[k]] += 1/distances[k]  \n",
    "                predictions[i] = np.argmax(count) \n",
    "                #In case of multiple occurrences of the maximum values, \n",
    "                #the indices corresponding to the first occurrence are returned\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "def euclidean_distance(x1, x2):\n",
    "    \"\"\"\n",
    "    Given vectors x1 and x2 with shape (n,) returns distance between vectors as float.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((x1 - x2)*(x1 - x2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Evaluation (25 Points)\n",
    "\n",
    "1) *(10 Points)* Implement functions to compute precision, recall and F1-score. `y_pred` and `y_true` are the vectors of predicted and true class labels respectively with shape `(n,)`, where `n` is the number of samples. Each function should return a float containing the corresponding score. It is advisable to implement a function for the confusion matrix and reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionMatrix(y_pred, y_true):\n",
    "    conMat = np.zeros((3,3))\n",
    "    for i in range(len(y_pred)):\n",
    "        conMat[y_pred[i],y_true[i]]+=1\n",
    "    return conMat\n",
    "\n",
    "#Amount of rightly labeled data points divided by the number of total data points we labeled with index i\n",
    "def precision(y_pred, y_true):  \n",
    "    conMat = confusionMatrix(y_pred,y_true)\n",
    "    prec = np.zeros((3)) #We thought for precision it doesnt make sense to return a scalar\n",
    "    for i in range(3):\n",
    "        prec[i] = conMat[i,i]/sum(conMat[:,i])\n",
    "    return prec\n",
    "\n",
    "#Amount of rightly labeled data points divided by the number of total data points that truly have this label\n",
    "def recall(y_pred, y_true):\n",
    "    conMat = confusionMatrix(y_pred,y_true)\n",
    "    rec = np.zeros((3)) #We thought for recall it doesnt make sense to return a scalar\n",
    "    for i in range(3):\n",
    "        rec[i] = conMat[i,i]/sum(conMat[i,:])\n",
    "    return rec\n",
    "\n",
    "def f1score(y_pred, y_true): #This should be a matrix\n",
    "    r = recall(y_pred, y_true)\n",
    "    p = precision(y_pred, y_true)\n",
    "    f1 = np.zeros((3,3))\n",
    "    for i in range(3):\n",
    "        for k in range(3):\n",
    "            f1[i,k] = 2*r[i]*p[k]/(r[i]+p[k])\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) *(10 Points)* Evaluate the performance of kNN with uniform weighting on the Wine dataset for `k=1,5,9`. Train each of the `3` classifiers on the training data from Task 1. Perform the predictions on both the training and test data. Then compute precision, recall, and F1-score for each model and for both training and test data. Visualize the performance in a plot, what do you observe?\n",
    "\n",
    "**For all students other than B.Sc. Data Science:** \n",
    "\n",
    "Also evaluate the kNN classifier with Euclidean distance-weighting. Compare the performance to uniform-weighting. How does the performance change compared to uniform weighting for each `k`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the prediction for the wine with  1 .-Nearest Neighbor\n",
      "We use distance to evaluate it\n",
      "   This is the precision for wine  0  :  1.0\n",
      "   This is the recall for wine     0  :  0.8125\n",
      "   This is the f1 score for wine   0  :  [0.89655172 0.80194805 0.89655172]\n",
      "   This is the precision for wine  1  :  0.7916666666666666\n",
      "   This is the recall for wine     1  :  1.0\n",
      "   This is the f1 score for wine   1  :  [1.         0.88372093 1.        ]\n",
      "   This is the precision for wine  2  :  1.0\n",
      "   This is the recall for wine     2  :  0.8947368421052632\n",
      "   This is the f1 score for wine   2  :  [0.94444444 0.84005202 0.94444444]\n",
      "_________________________________________________\n",
      "The average precision is  0.9305555555555555\n",
      "The average recall is     0.9024122807017544\n",
      "The average f1score is    0.9119681483277825\n",
      "_________________________________________________\n",
      "\n",
      "--------------------------------------------------------------\n",
      "This is the prediction for the wine with  5 .-Nearest Neighbor\n",
      "We use distance to evaluate it\n",
      "   This is the precision for wine  0  :  1.0\n",
      "   This is the recall for wine     0  :  0.9285714285714286\n",
      "   This is the f1 score for wine   0  :  [0.96296296 0.87837838 0.96296296]\n",
      "   This is the precision for wine  1  :  0.8333333333333334\n",
      "   This is the recall for wine     1  :  1.0\n",
      "   This is the f1 score for wine   1  :  [1.         0.90909091 1.        ]\n",
      "   This is the precision for wine  2  :  1.0\n",
      "   This is the recall for wine     2  :  0.85\n",
      "   This is the f1 score for wine   2  :  [0.91891892 0.84158416 0.91891892]\n",
      "_________________________________________________\n",
      "The average precision is  0.9444444444444445\n",
      "The average recall is     0.9261904761904761\n",
      "The average f1score is    0.9325352455165437\n",
      "_________________________________________________\n",
      "\n",
      "--------------------------------------------------------------\n",
      "This is the prediction for the wine with  9 .-Nearest Neighbor\n",
      "We use distance to evaluate it\n",
      "   This is the precision for wine  0  :  1.0\n",
      "   This is the recall for wine     0  :  0.8666666666666667\n",
      "   This is the f1 score for wine   0  :  [0.92857143 0.82747069 0.92857143]\n",
      "   This is the precision for wine  1  :  0.7916666666666666\n",
      "   This is the recall for wine     1  :  1.0\n",
      "   This is the f1 score for wine   1  :  [1.         0.88372093 1.        ]\n",
      "   This is the precision for wine  2  :  1.0\n",
      "   This is the recall for wine     2  :  0.85\n",
      "   This is the f1 score for wine   2  :  [0.91891892 0.81979695 0.91891892]\n",
      "_________________________________________________\n",
      "The average precision is  0.9305555555555555\n",
      "The average recall is     0.9055555555555556\n",
      "The average f1score is    0.9139965851439046\n",
      "_________________________________________________\n",
      "\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "k = [1,5,9]\n",
    "\n",
    "\n",
    "def test(x,y):\n",
    "    p = precision(x,y)\n",
    "    r = recall(x,y)\n",
    "    f = f1score(x,y)\n",
    "    pTotal = sum(p)/3\n",
    "    rTotal = sum(r)/3\n",
    "    fTotal = sum(sum(f))/9\n",
    "    for j in range(3):\n",
    "        print(\"   This is the precision for wine \", j, \" : \", p[j])\n",
    "        print(\"   This is the recall for wine    \", j, \" : \", r[j])\n",
    "        print(\"   This is the f1 score for wine  \", j, \" : \", f[j])\n",
    "    print(\"_________________________________________________\")\n",
    "    print(\"The average precision is \", pTotal)\n",
    "    print(\"The average recall is    \", rTotal)\n",
    "    print(\"The average f1score is   \", fTotal)\n",
    "    print(\"_________________________________________________\")\n",
    "    print()\n",
    "    \n",
    "    \n",
    "for i in k:\n",
    "    print(\"This is the prediction for the wine with \", i, \".-Nearest Neighbor\")\n",
    "    print(\"We use distance to evaluate it\")\n",
    "    KNN = KNearestNeighbors(i, \"distance\")\n",
    "    KNN.fit(X_train, y_train)\n",
    "    y_pred = KNN.predict(X_test)\n",
    "    test(y_pred, y_test)\n",
    "    \n",
    "    print(\"--------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With k=1, the predictions always are very clear as it only suggests one label. After running the program several times, one notices that it varies heavily if the average precision and average recall are best in case of k=1, k=5 or k=9.  Still, after checking the results for k=90 (uniform), one sees that the results get very bad.  Overall, when considering the distances as well, the results get better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) *(5 Points)* Explain why kNN with `k=1` achieves perfect results on the training data. Why is it not the best model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN with k=1 achieves perfect results, because the model is fitted perfectly on the training data. With k=1, each point is assigned exactly the label of the one closest point in the training set, which obviously is itself. Therefore, the loss is very small. Still, this is a classical case of the so called Overfitting, where the training data is represented perfectly, but it's performing badly on unknown test data. The model is built to represent exactly the training data, but it does not generalize very well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
